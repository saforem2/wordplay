{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc5d004-5e6f-4bb3-938e-fb44f919cec3",
   "metadata": {},
   "source": [
    "# `nanoGPT`: GPT-2 XL (1.5B Params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd994b32-ca5f-4e00-81d3-89dbbf7b2093",
   "metadata": {},
   "source": [
    "## Install / Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c38d27-5acb-4e5d-87c8-dbb2739cf277",
   "metadata": {},
   "source": [
    "### First Time Running\n",
    "\n",
    "We need to install `ngpt` and setup the Shakespeare dataset\n",
    "\n",
    "This will need to be ran the first time you are running this notebook.\n",
    "\n",
    "Following the\n",
    "\n",
    "```python\n",
    "!python3 -m pip install nanoGPT\n",
    "```\n",
    "\n",
    "you will need to restart your runtime (Runtime -> Restart runtime)\n",
    "\n",
    "After this, you should be able to\n",
    "\n",
    "```python\n",
    ">>> import ngpt\n",
    ">>> ngpt.__file__\n",
    "'/content/nanoGPT/src/ngpt/__init__.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea233681-405f-4754-a394-e8e6c57080df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T14:04:38.696624Z",
     "iopub.status.busy": "2023-11-30T14:04:38.696401Z",
     "iopub.status.idle": "2023-11-30T14:04:38.815740Z",
     "shell.execute_reply": "2023-11-30T14:04:38.815336Z",
     "shell.execute_reply.started": "2023-11-30T14:04:38.696604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\n",
      "Has ngpt installed. Nothing to do.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 -c 'import ngpt; print(ngpt.__file__)' 2> '/dev/null'\n",
    "\n",
    "if [[ $? -eq 0 ]]; then\n",
    "    echo \"Has ngpt installed. Nothing to do.\"\n",
    "else\n",
    "    echo \"Does not have ngpt installed. Installing...\"\n",
    "    git clone 'https://github.com/saforem2/nanoGPT'\n",
    "    python3 nanoGPT/data/shakespeare_char/prepare.py\n",
    "    python3 -m pip install -e nanoGPT -vvv\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14a8da7-72fe-4839-a14d-f01606285fc3",
   "metadata": {},
   "source": [
    "## Post Install\n",
    "\n",
    "If installed correctly, you should be able to:\n",
    "\n",
    "```python\n",
    ">>> import ngpt\n",
    ">>> ngpt.__file__\n",
    "'/path/to/nanoGPT/src/ngpt/__init__.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5267757b-d817-4861-9320-a0ca1f47e465",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T14:04:45.572626Z",
     "iopub.status.busy": "2023-11-30T14:04:45.572143Z",
     "iopub.status.idle": "2023-11-30T14:04:45.765630Z",
     "shell.execute_reply": "2023-11-30T14:04:45.765110Z",
     "shell.execute_reply.started": "2023-11-30T14:04:45.572607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:04:45]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119m3434626787.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m7\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35m__init__.py\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ngpt\n",
    "from enrich import get_logger\n",
    "log = get_logger('jupyter')\n",
    "log.info(ngpt.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e329ed94-582e-4aa8-bbe0-ba56782fe9e9",
   "metadata": {},
   "source": [
    "## Build Trainer\n",
    "\n",
    "Explicitly, we:\n",
    "\n",
    "1. `setup_torch(...)`\n",
    "2. Build `cfg: DictConfig = get_config(...)`\n",
    "3. Instnatiate `config: ExperimentConfig = instantiate(cfg)`\n",
    "4. Build `trainer = Trainer(config)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d12405a-8f88-476b-8922-a1a212adc682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T14:34:50.662850Z",
     "iopub.status.busy": "2023-11-30T14:34:50.662431Z",
     "iopub.status.idle": "2023-11-30T14:35:26.701388Z",
     "shell.execute_reply": "2023-11-30T14:35:26.701067Z",
     "shell.execute_reply.started": "2023-11-30T14:34:50.662832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SEED: <span style=\"color: #800080; text-decoration-color: #800080\">822569775</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SEED: \u001b[35m822569775\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-30 08:34:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m295\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Loading val from \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/data/openwebtext/\u001b[0m\u001b[35mval.bin\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:34:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m295\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Loading train from \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/data/openwebtext/\u001b[0m\u001b[35mtrain.bin\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:34:50]\u001b[0m\u001b[33m[WARNING]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m330\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - No meta.pkl found, assuming GPT-\u001b[35m2\u001b[0m encodings\u001b[33m...\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:34:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m270\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Rescaling GAS -> GAS \u001b[32m/\u001b[0m\u001b[32m/\u001b[0m WORLD_SIZE = \u001b[35m1\u001b[0m \u001b[32m/\u001b[0m\u001b[32m/\u001b[0m \u001b[35m1\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:34:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Tokens per iteration: \u001b[35m64\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:34:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m454\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Using \u001b[1m<\u001b[0m\u001b[1;95mtorch.amp.autocast_mode.autocast\u001b[0m\u001b[39m object at \u001b[0m\u001b[35m0x7f08ed0a7370\u001b[0m\u001b[1m>\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:34:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mtrainer.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m187\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Initializing from OpenAI GPT-\u001b[35m2\u001b[0m Weights: gpt2-xl\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:34:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m225\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - loading weights from pretrained gpt: gpt2-xl\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:34:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m234\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - forcing \u001b[3;94mvocab_size\u001b[0m=\u001b[35m50257\u001b[0m, \u001b[3;94mblock_size\u001b[0m=\u001b[35m1024\u001b[0m, \u001b[3;94mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:34:50]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m240\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - overriding dropout rate to \u001b[35m0.0\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:35:10]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m160\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - number of parameters: \u001b[35m1555.\u001b[0m97M\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:35:26]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m290\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - num decayed parameter tensors: \u001b[35m194\u001b[0m, with \u001b[35m1\u001b[0m,\u001b[35m556\u001b[0m,\u001b[35m609\u001b[0m,\u001b[35m600\u001b[0m parameters\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:35:26]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m291\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - num non-decayed parameter tensors: \u001b[35m386\u001b[0m, with \u001b[35m1\u001b[0m,\u001b[35m001\u001b[0m,\u001b[35m600\u001b[0m parameters\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:35:26]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m297\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - using fused AdamW: \u001b[3;92mTrue\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from ezpz import setup_torch\n",
    "from hydra.utils import instantiate\n",
    "from ngpt.configs import get_config, PROJECT_ROOT\n",
    "from ngpt.trainer import Trainer\n",
    "from enrich.console import get_console\n",
    "\n",
    "console = get_console()\n",
    "HF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\n",
    "HF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "os.environ['MASTER_PORT'] = '5127'\n",
    "os.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n",
    "\n",
    "SEED = np.random.randint(2**32)\n",
    "console.print(f'SEED: {SEED}')\n",
    "\n",
    "rank = setup_torch('DDP', seed=1234)\n",
    "cfg = get_config(\n",
    "    [\n",
    "        'data=owt',\n",
    "        'model=gpt2_xl',\n",
    "        'model.block_size=64',\n",
    "        'optimizer=gpt2_xl',\n",
    "        'train=gpt2_xl',\n",
    "        'train.init_from=gpt2-xl',\n",
    "        'train.max_iters=100',\n",
    "        'train.dtype=bfloat16',\n",
    "    ]\n",
    ")\n",
    "config = instantiate(cfg)\n",
    "trainer = Trainer(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6d7193-d882-4cb5-b9ad-20813b1a3ea2",
   "metadata": {},
   "source": [
    "## Prompt (**prior** to training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98572eab-1387-4a63-a76d-8e55737be801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T14:35:26.705240Z",
     "iopub.status.busy": "2023-11-30T14:35:26.705027Z",
     "iopub.status.idle": "2023-11-30T14:35:54.456675Z",
     "shell.execute_reply": "2023-11-30T14:35:54.456270Z",
     "shell.execute_reply.started": "2023-11-30T14:35:26.705223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-30 08:35:54]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119m1657463709.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m3\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[1m[\u001b[0m\u001b[32m'prompt'\u001b[0m\u001b[1m]\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mquery\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-30 08:35:54]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119m1657463709.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m4\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[1m[\u001b[0m\u001b[32m'response'\u001b[0m\u001b[1m]\u001b[0m:\n",
      "\n",
      "What is a supercomputer? When it comes to massive computing, a supercomputer is simply a large computer system that has the ability to perform many calculations at once. This can be the result of using many different processing cores, or memory, or operating at a high clock speed. Supercomputers are often used to crack complex calculations and research problems.\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "On a larger scale, these massive computers are used to solve tough mathematical equations and solve hard scientific problems. They are very powerful enough to emulate the workings of the human brain and simulate a human intelligence in a virtual world.\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "In \u001b[35m1992\u001b[0m, IBM's NeXTStep supercomputer was the largest and most powerful supercomputer in the world. It was released in \u001b[35m1995\u001b[0m and did not continue to live up to its original promises, because its capabilities were quickly surpassed by its competitors.\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\n",
      "\n",
      "Image credit: Wikipedia\u001b[1m<\u001b[0m\u001b[1;95m|endoftext|\u001b[0m\u001b[1m>\u001b[0mEditor's note: Dan De Luce is the author of \u001b[32m\"When the Going Gets Tough: The New Survival Guide for College Students and Your Health and Well-Being.\"\u001b[0m\n",
      "\n",
      "College has never been more expensive. But with so many choices and so many choices of where to go, it's harder than ever for prospective students to find a college that fits their lifestyle.\n",
      "\n",
      "This is a problem—not just because it can be a hassle to find a college that doesn't require a large amount of financial aid. It's a problem because it can be costly for students to stay in college.\n",
      "\n",
      "So I created this list of colleges with the highest tuition where\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a supercomputer?\"\n",
    "outputs = trainer.evaluate(query, num_samples=1, display=False)\n",
    "log.info(\"['prompt']: '{query}'\")\n",
    "log.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5202ff3-8811-47c9-8b9d-9818d4603697",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "\n",
    "|  **NAME**  |     **DESCRIPTION**          |\n",
    "|:----------:|:----------------------------:|\n",
    "|   `step`   | Current training step        |\n",
    "|   `loss`   | Loss value                   |\n",
    "|   `dt`     | Time per step (in **ms**)    |\n",
    "|   `sps`    | Samples per second           |\n",
    "|   `mtps`   | (million) Tokens per sec     |\n",
    "|   `mfu`    | Model Flops Utilization*     |\n",
    "^Logging Legend\n",
    "\n",
    "*in units of A100 `bfloat16` peak FLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acb8d33b-f8c9-41c6-aa61-95bdb76bf6ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T15:17:05.496214Z",
     "iopub.status.busy": "2023-11-30T15:17:05.495029Z",
     "iopub.status.idle": "2023-11-30T15:19:13.322378Z",
     "shell.execute_reply": "2023-11-30T15:19:13.307010Z",
     "shell.execute_reply.started": "2023-11-30T15:17:05.496193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8892ebfe8bcc467b86312fa4d5f890a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacty of 79.35 GiB of which 48.19 MiB is free. Including non-PyTorch memory, this process has 79.30 GiB memory in use. Of the allocated memory 77.64 GiB is allocated by PyTorch, and 225.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/trainer.py:476\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_iters)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39miter_num \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbest_val_loss\n\u001b[1;32m    473\u001b[0m              \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39malways_save_checkpoint)\n\u001b[1;32m    474\u001b[0m     ):\n\u001b[1;32m    475\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_ckpt(add_to_wandb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 476\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    478\u001b[0m dt \u001b[38;5;241m=\u001b[39m t1 \u001b[38;5;241m-\u001b[39m t0\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/trainer.py:374\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# NOTE: -----------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# In DDP training we only need to sync gradients at the last micro\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# step. the official way to do this is with model.no_sync() context\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# manager, it just toggles this variable\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    369\u001b[0m _ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mrequire_backward_grad_sync\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (is_last_micro_step \u001b[38;5;129;01mand\u001b[39;00m WORLD_SIZE \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    373\u001b[0m )\n\u001b[0;32m--> 374\u001b[0m fout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# immediately async prefetch next batch while model is doing the forward pass on the GPU\u001b[39;00m\n\u001b[1;32m    376\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/trainer.py:317\u001b[0m, in \u001b[0;36mTrainer._forward_step\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    315\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mctx:\n\u001b[0;32m--> 317\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m: logits,\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss,\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdt\u001b[39m\u001b[38;5;124m'\u001b[39m: time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    322\u001b[0m }\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/parallel/distributed.py:1519\u001b[0m, in \u001b[0;36mDistributedDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistributedDataParallel.forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1515\u001b[0m     inputs, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_forward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1516\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_delay_all_reduce_all_params\n\u001b[0;32m-> 1519\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_ddp_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1520\u001b[0m     )\n\u001b[1;32m   1521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_forward(output)\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/parallel/distributed.py:1355\u001b[0m, in \u001b[0;36mDistributedDataParallel._run_ddp_forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_ddp_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inside_ddp_forward():\n\u001b[0;32m-> 1355\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.py:193\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    191\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdrop(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 193\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# if we are given some desired targets also calculate the loss\u001b[39;00m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.py:127\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    126\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x))\n\u001b[0;32m--> 127\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/model.py:111\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    110\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(x)\n\u001b[0;32m--> 111\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(x)\n\u001b[1;32m    113\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/activation.py:682\u001b[0m, in \u001b[0;36mGELU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproximate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproximate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacty of 79.35 GiB of which 48.19 MiB is free. Including non-PyTorch memory, this process has 79.30 GiB memory in use. Of the allocated memory 77.64 GiB is allocated by PyTorch, and 225.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.model.module.train()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d690f-eb0b-45cb-9459-1b5a4e9ebf33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba05f8-d33b-4d80-85ca-04f155429394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa5c0c5a-539a-462d-a9e3-0655e8e48d85",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca74f02-4e8b-4390-bac7-68fe5c1a3c40",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-30T14:30:34.064970Z",
     "iopub.status.idle": "2023-11-30T14:30:34.065170Z",
     "shell.execute_reply": "2023-11-30T14:30:34.065075Z",
     "shell.execute_reply.started": "2023-11-30T14:30:34.065066Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"What is a supercomputer?\"\n",
    "outputs = trainer.evaluate(query, num_samples=1, display=False)\n",
    "log.info(\"['prompt']: '{query}'\")\n",
    "log.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4656d0-2d93-430c-80b2-2123d5f27acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
