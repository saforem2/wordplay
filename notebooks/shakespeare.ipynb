{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1d841301-7152-45fd-836f-d0dda350fd57",
   "metadata": {},
   "source": [
    "---\n",
    "title: title\n",
    "author: author\n",
    "execute:\n",
    "    enabled: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc5d004-5e6f-4bb3-938e-fb44f919cec3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-11T07:14:30.136832+00:00",
     "iopub.status.idle": "2023-11-11T07:14:30.175293+00:00"
    }
   },
   "source": [
    "# `nanoGPT`: Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd994b32-ca5f-4e00-81d3-89dbbf7b2093",
   "metadata": {},
   "source": [
    "## Install / Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a8bdbf-d488-4553-8955-9a2ad01bc54b",
   "metadata": {},
   "source": [
    "### Google Colab\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c38d27-5acb-4e5d-87c8-dbb2739cf277",
   "metadata": {},
   "source": [
    "### First Time Running\n",
    "\n",
    "We need to install `ngpt` and setup the Shakespeare dataset\n",
    "\n",
    "This will need to be ran the first time you are running this notebook.\n",
    "\n",
    "Following the\n",
    "\n",
    "```python\n",
    "!python3 -m pip install nanoGPT\n",
    "```\n",
    "\n",
    "you will need to restart your runtime (Runtime -> Restart runtime)\n",
    "\n",
    "After this, you should be able to\n",
    "\n",
    "```python\n",
    ">>> import ngpt\n",
    ">>> ngpt.__file__\n",
    "'/content/nanoGPT/src/ngpt/__init__.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e60f3e",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T16:35:09.547786Z",
     "iopub.status.busy": "2023-11-30T16:35:09.547243Z",
     "iopub.status.idle": "2023-11-30T16:35:09.697821Z",
     "shell.execute_reply": "2023-11-30T16:35:09.697442Z",
     "shell.execute_reply.started": "2023-11-30T16:35:09.547769Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/__init__.py\n",
      "Has ngpt installed. Nothing to do.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 -c 'import ngpt; print(ngpt.__file__)' 2> '/dev/null'\n",
    "\n",
    "if [[ $? -eq 0 ]]; then\n",
    "    echo \"Has ngpt installed. Nothing to do.\"\n",
    "else\n",
    "    echo \"Does not have ngpt installed. Installing...\"\n",
    "    git clone 'https://github.com/saforem2/nanoGPT'\n",
    "    python3 nanoGPT/data/shakespeare_char/prepare.py\n",
    "    python3 -m pip install -e nanoGPT -vvv\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14a8da7-72fe-4839-a14d-f01606285fc3",
   "metadata": {},
   "source": [
    "## Post Install\n",
    "\n",
    "If installed correctly, you should be able to:\n",
    "\n",
    "```python\n",
    ">>> import ngpt\n",
    ">>> ngpt.__file__\n",
    "'/path/to/nanoGPT/src/ngpt/__init__.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fbf22d1-34ba-48ae-a78e-fc447fc9a0de",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T16:35:09.818095Z",
     "iopub.status.busy": "2023-11-30T16:35:09.817884Z",
     "iopub.status.idle": "2023-11-30T16:35:10.193029Z",
     "shell.execute_reply": "2023-11-30T16:35:10.192647Z",
     "shell.execute_reply.started": "2023-11-30T16:35:09.818079Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\u001b[38;2;131;131;131m[2023-11-30 10:35:10]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119m1583707193.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m8\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35m__init__.py\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ngpt\n",
    "from enrich import get_logger\n",
    "log = get_logger('jupyter')\n",
    "#from rich import print\n",
    "log.info(ngpt.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e329ed94-582e-4aa8-bbe0-ba56782fe9e9",
   "metadata": {},
   "source": [
    "## Build Trainer\n",
    "\n",
    "Explicitly, we:\n",
    "\n",
    "1. `setup_torch(...)`\n",
    "2. Build `cfg: DictConfig = get_config(...)`\n",
    "3. Instnatiate `config: ExperimentConfig = instantiate(cfg)`\n",
    "4. Build `trainer = Trainer(config)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d12405a-8f88-476b-8922-a1a212adc682",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T16:35:10.194033Z",
     "iopub.status.busy": "2023-11-30T16:35:10.193752Z",
     "iopub.status.idle": "2023-11-30T16:36:28.944914Z",
     "shell.execute_reply": "2023-11-30T16:36:28.943625Z",
     "shell.execute_reply.started": "2023-11-30T16:35:10.194017Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "WARNING: There was an error initializing an OpenFabrics device.\n",
      "\n",
      "  Local host:   thetagpu23\n",
      "  Local device: mlx5_0\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-30 10:35:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m72\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Setting HF_DATASETS_CACHE to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/.cache/huggingface/\u001b[0m\u001b[35mdatasets\u001b[0m\n",
      "Failed to download font: Source Sans Pro, skipping!\n",
      "Failed to download font: Titillium WebRoboto Condensed, skipping!\n",
      "\u001b[38;2;131;131;131m[2023-11-30 10:36:27]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m295\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Loading val from \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/data/shakespeare_char/\u001b[0m\u001b[35mval.bin\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-30 10:36:27]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m295\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Loading train from \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/data/shakespeare_char/\u001b[0m\u001b[35mtrain.bin\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-30 10:36:27]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m270\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Rescaling GAS -> GAS \u001b[32m/\u001b[0m\u001b[32m/\u001b[0m WORLD_SIZE = \u001b[35m1\u001b[0m \u001b[32m/\u001b[0m\u001b[32m/\u001b[0m \u001b[35m1\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-30 10:36:27]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Tokens per iteration: \u001b[35m16\u001b[0m,\u001b[35m384\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-30 10:36:27]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m454\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Using \u001b[1m<\u001b[0m\u001b[1;95mtorch.amp.autocast_mode.autocast\u001b[0m\u001b[39m object at \u001b[0m\u001b[35m0x7efbdc19b100\u001b[0m\u001b[1m>\u001b[0m\n",
      "\u001b[38;2;131;131;131m[2023-11-30 10:36:27]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mconfigs.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m460\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Initializing a new model from scratch\n",
      "\u001b[38;2;131;131;131m[2023-11-30 10:36:27]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mtrainer.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m179\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Initializing a new model from scratch\n",
      "\u001b[38;2;131;131;131m[2023-11-30 10:36:28]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[\u001b[0m\u001b[38;2;119;119;119mmodel.py\u001b[0m\u001b[38;2;119;119;119m:\u001b[0m\u001b[38;2;119;119;119m160\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - number of parameters: \u001b[35m10.\u001b[0m65M\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m\n\u001b[1;32m     14\u001b[0m cfg \u001b[38;5;241m=\u001b[39m get_config(\n\u001b[1;32m     15\u001b[0m     [\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata=shakespeare\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m config \u001b[38;5;241m=\u001b[39m instantiate(cfg)\n\u001b[0;32m---> 27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/trainer.py:198\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, GPT)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(GPT, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/venvs/thetaGPU/2023-04-26/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from ezpz import setup_torch\n",
    "from hydra.utils import instantiate\n",
    "from ngpt.configs import get_config, PROJECT_ROOT\n",
    "from ngpt.trainer import Trainer\n",
    "HF_DATASETS_CACHE = PROJECT_ROOT.joinpath('.cache', 'huggingface')\n",
    "HF_DATASETS_CACHE.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "os.environ['MASTER_PORT'] = '5671'\n",
    "os.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE.as_posix()\n",
    "\n",
    "rank = setup_torch('DDP', seed=1234)\n",
    "cfg = get_config(\n",
    "    [\n",
    "        'data=shakespeare',\n",
    "        'model=shakespeare',\n",
    "        'optimizer=shakespeare',\n",
    "        'train=shakespeare',\n",
    "        'train.dtype=bfloat16',\n",
    "        'train.max_iters=5000',\n",
    "        'train.log_interval=250',\n",
    "        'train.eval_interval=1000',\n",
    "    ]\n",
    ")\n",
    "config = instantiate(cfg)\n",
    "trainer = Trainer(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca969e9-5132-42a8-9728-9db1bce6a198",
   "metadata": {},
   "source": [
    "## Prompt (**prior** to training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a42517f-d758-43b7-97d9-92fc7a2d0613",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-30T16:36:28.945607Z",
     "iopub.status.idle": "2023-11-30T16:36:28.945809Z",
     "shell.execute_reply": "2023-11-30T16:36:28.945713Z",
     "shell.execute_reply.started": "2023-11-30T16:36:28.945704Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"What is a supercomputer?\"\n",
    "outputs = trainer.evaluate(query, num_samples=1, display=False)\n",
    "log.info(\"['prompt']: '{query}'\")\n",
    "log.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5202ff3-8811-47c9-8b9d-9818d4603697",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "|  name  |       description            |\n",
    "|:------:|:----------------------------:|\n",
    "| `step` | Current training step        |\n",
    "| `loss` | Loss value                   |\n",
    "| `dt`   | Time per step (in **ms**)    |\n",
    "| `sps`  | Samples per second           |\n",
    "| `mtps` | (million) Tokens per sec     |\n",
    "| `mfu`  | Model Flops utilization[^1]  |\n",
    "^legend: #tbl-legend\n",
    "\n",
    "[^1]: in units of A100 `bfloat16` peak FLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb8d33b-f8c9-41c6-aa61-95bdb76bf6ea",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-30T16:36:28.946773Z",
     "iopub.status.idle": "2023-11-30T16:36:28.946965Z",
     "shell.execute_reply": "2023-11-30T16:36:28.946874Z",
     "shell.execute_reply.started": "2023-11-30T16:36:28.946865Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:16:47]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m1750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.175\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.460\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.417\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.597\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.659\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:16:54]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.140\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.889\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.190\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.609\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.678\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.299\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m4\u001b[0m\u001b[35m.291\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:16:58]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:16:58]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:16:58]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:17:05]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.121\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.308\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.619\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.600\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.675\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:17:12]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.067\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.838\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.261\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.610\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:17:19]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m2750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.034\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.360\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.550\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.599\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.688\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:17:26]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.009\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.237\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.114\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.740\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:17:33]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.940\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.991\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.607\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.746\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:17:39]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.947\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.261\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.080\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.791\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:17:46]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m3750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.885\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.216\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.870\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.440\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.413\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:17:53]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.866\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.241\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.108\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.492\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.050\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.474\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:17:57]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:17:57]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:17:57]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:18:04]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.847\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.228\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.728\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.602\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.511\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:18:11]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.835\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.215\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.147\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.625\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.581\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:18:18]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m4750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.822\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.657\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.513\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.615\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.621\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:18:24]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m5000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.808\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.635\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.544\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.615\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.658\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:18:31]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m5250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.811\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.267\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.071\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.711\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:18:38]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m5500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.769\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.406\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.870\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.620\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.751\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:18:44]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m5750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.780\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.239\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.111\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.796\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:18:51]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m6000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.767\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.682\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.478\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.614\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.813\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.696\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.637\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:18:55]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:18:55]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:18:56]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:19:02]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m6250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.773\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m31\u001b[0m\u001b[35m.104\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m32\u001b[0m\u001b[35m.151\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.527\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.629\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:19:09]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m6500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.759\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.142\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.843\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.604\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.639\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:19:16]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m6750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.753\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.712\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.437\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.613\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.670\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:19:22]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m7000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.745\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.871\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.215\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.610\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.690\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:19:29]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m7250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.733\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.266\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.072\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.624\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.740\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:19:36]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m7500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.723\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.817\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.289\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.611\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.755\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:19:43]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m7750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.747\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.461\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.791\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.619\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.788\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:19:49]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m8000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.729\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m29\u001b[0m\u001b[35m.348\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m34\u001b[0m\u001b[35m.074\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.558\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.679\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.556\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.755\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:19:53]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:432\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving checkpoint to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:19:53]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:433\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Saving model to: \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/\u001b[0m\u001b[35mmodel.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:19:54]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[configs.py:129\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - Appending \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/\u001b[0m\u001b[35mngpt\u001b[0m to \u001b[32m/lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/nanoGPT/src/ngpt/ckpts/\u001b[0m\u001b[35mcheckpoints.log\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:20:01]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m8250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.718\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.464\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.787\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.619\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.719\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:20:07]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m8500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.705\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.051\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.967\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.606\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.725\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:20:14]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m8750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.704\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.298\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.026\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.623\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.769\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:20:21]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m9000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.694\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.131\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.858\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.604\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.766\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:20:27]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m9250\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.700\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.291\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m38\u001b[0m\u001b[35m.036\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.623\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.806\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:20:34]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m9500\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.668\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m27\u001b[0m\u001b[35m.353\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m36\u001b[0m\u001b[35m.560\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.599\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.788\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:20:41]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m9750\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.658\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.422\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.847\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.620\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.819\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;131;131;131m[2023-11-11 01:20:48]\u001b[0m\u001b[34m[INFO]\u001b[0m\u001b[38;2;119;119;119m[trainer.py:516\u001b[0m\u001b[38;2;119;119;119m]\u001b[0m - \u001b[3;94mstep\u001b[0m=\u001b[35m10000\u001b[0m \u001b[3;94mloss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.678\u001b[0m \u001b[3;94mdt\u001b[0m=\u001b[35m26\u001b[0m\u001b[35m.887\u001b[0m \u001b[3;94msps\u001b[0m=\u001b[35m37\u001b[0m\u001b[35m.192\u001b[0m \u001b[3;94mmtps\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.609\u001b[0m \u001b[3;94mmfu\u001b[0m=\u001b[35m13\u001b[0m\u001b[35m.823\u001b[0m \u001b[3;94mtrain_loss\u001b[0m=\u001b[35m0\u001b[0m\u001b[35m.473\u001b[0m \u001b[3;94mval_loss\u001b[0m=\u001b[35m1\u001b[0m\u001b[35m.840\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c0c5a-539a-462d-a9e3-0655e8e48d85",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa61f9-8217-47b4-b885-b193c0955a7b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-30T16:36:29.456635Z",
     "iopub.status.idle": "2023-11-30T16:36:29.457001Z",
     "shell.execute_reply": "2023-11-30T16:36:29.456895Z",
     "shell.execute_reply.started": "2023-11-30T16:36:29.456884Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"Who is Romeo?\"\n",
    "outputs = trainer.evaluate(query, num_samples=1, display=False)\n",
    "log.info(\"['prompt']: '{query}'\")\n",
    "log.info(\"['response']:\\n\\n\" + fr\"{outputs['0']['raw']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6fe237-9c36-4b19-80fb-d9c6293d647f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "09608c829e3c41cfb9df32a38bc540a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "buffers": [],
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "29e17460a7de4b139111d12c7d3569dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "buffers": [],
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2dbde29da70f444a92bc7da43bbb47f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "buffers": [],
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "36a4f9becb434320ab8c4d625e5e4ab7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "buffers": [],
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e2dbd76a061447b8f79ece8f2679cb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "buffers": [],
       "description_width": ""
      }
     },
     "83406e2a819f4ac4bce6aebe72d802f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "buffers": [],
       "children": [
        "IPY_MODEL_becff553ccd54cceb5467f0a96cb00c7",
        "IPY_MODEL_f1d80a4ac2594494a970534573cd0f5e",
        "IPY_MODEL_e1b0c5d54eaa4791851f5a8744f463d8"
       ],
       "layout": "IPY_MODEL_36a4f9becb434320ab8c4d625e5e4ab7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "becff553ccd54cceb5467f0a96cb00c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "buffers": [],
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_29e17460a7de4b139111d12c7d3569dd",
       "placeholder": "",
       "style": "IPY_MODEL_09608c829e3c41cfb9df32a38bc540a2",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "ca8984b34602451fbe107f25eec68282": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "buffers": [],
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d3474b1b5ecc4e669b7d12b736096303": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "buffers": [],
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1b0c5d54eaa4791851f5a8744f463d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "buffers": [],
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d3474b1b5ecc4e669b7d12b736096303",
       "placeholder": "",
       "style": "IPY_MODEL_2dbde29da70f444a92bc7da43bbb47f2",
       "tabbable": null,
       "tooltip": null,
       "value": " 10000/10000 [04:53&lt;00:00, 37.57it/s]"
      }
     },
     "f1d80a4ac2594494a970534573cd0f5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "buffers": [],
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ca8984b34602451fbe107f25eec68282",
       "max": 10000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7e2dbd76a061447b8f79ece8f2679cb3",
       "tabbable": null,
       "tooltip": null,
       "value": 10000
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
