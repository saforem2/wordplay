[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "wordplay 🎮 💬",
    "section": "",
    "text": "Playing with words.\nA set of simple, scalable and highly configurable tools for working1 with LLMs."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "wordplay 🎮 💬",
    "section": "Background",
    "text": "Background\nWhat started as some simple modifications to Andrej Karpathy's nanoGPT has now grown into the wordplay project.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: nanoGPT\n\n\n\n\n\n\n\n\n\n\nFigure 2: wordplay\n\n\n\n\n\n\n\nIf you’re curious…\n\nWhile nanoGPT is a great project and an excellent resource; it is, by design, very minimal2 and limited in its flexibility.\nWorking through the code I found myself making minor changes here and there to test new ideas and run variations on different experiments. These changes eventually built to the point where my {goals, scope, code} for the project had diverged significantly from the original vision.\nAs a result, I figured it made more sense to move things to a new project, wordplay.\nI’ve priortized adding functionality that I have found to be useful or interesting, but am absolutely open to input or suggestions for improvement.\nDifferent aspects of this project have been motivated by some of my recent work on LLMs.\n\nProjects:\n\nezpz: Painless distributed training with your favorite {framework, backend} combo.\nMegatron-DeepSpeed: Ongoing research training transformer language models at scale, including: BERT & GPT-2\n\nCollaboration(s):\n\nDeepSpeed4Science (2023-09)\n\nLoooooooong Sequence Lengths\nProject Website\nPreprint Song et al. (2023)\nBlog Post\nTutorial\n\nGenSLMs:\n\nGitHub\nPreprint\n🏆 ACM Gordon Bell Special Prize for COVID-19 Research\n\n\nTalks / Workshops:\n\nLLM-lunch-talk (2023-10-12): LLMs at ALCF.\n\nSlides\nGitHub\n\nCreating Small(-ish) LLMs (2023-11-30)\n\nWorkshop\nSlides\nGitHub"
  },
  {
    "objectID": "index.html#completed",
    "href": "index.html#completed",
    "title": "wordplay 🎮 💬",
    "section": "Completed",
    "text": "Completed\n\nWork with any 🤗 HuggingFace dataset\nEffortless distributed training using ezpz\nImproved (type-safe) and extensible configuration system (powered by hydra), see #config\nAutomatic, detailed experiment + metric tracking with Weights & Biases\n\nExample Workspace\nExample Run\n\nRich informative logging with enrich"
  },
  {
    "objectID": "index.html#in-progress",
    "href": "index.html#in-progress",
    "title": "wordplay 🎮 💬",
    "section": "In Progress",
    "text": "In Progress\n\nDeepSpeed support\nFull-Sharded Data-Parallel (FSDP) support\n\nIntroducing PyTorch Fully Sharded Data Parallel (FSDP) API | PyTorch\n\n3D Parallelism support via:\n\nMegatron-DeepSpeed\nnative PyTorch:\n\nPipeline Parallelism — PyTorch 2.1 documentation\npytorch/PiPPy: Pipeline Parallelism for PyTorch"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "wordplay 🎮 💬",
    "section": "Install",
    "text": "Install\n\n\nGrab-n-Go\n\nThe easiest way to get the most recent version is to:\npython3 -m pip install \"git+https://github.com/saforem2/wordplay.git\"\n\n\n\nDevelopment\n\nIf you’d like to work with the project and run / change things yourself, I’d recommend installing from a local (editable) clone of this repository:\ngit clone \"https://github.com/saforem2/wordplay\"\ncd wordplay\nmkdir v venv\npython3 -m venv venv --system-site-packages\nsource venv/bin/activate\npython3 -m pip install -e .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLast Updated: 12/20/2023 @ 09:11:10"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "wordplay 🎮 💬",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\n{\n  \"training\",\n  \"fine-tuning\",\n  \"benchmarking\",\n  \"parallelizing\",\n  \"distributing\",\n  \"measuring\",\n  \"...\"\n}\nlarge models at scale.↩︎\nnano, even 😂↩︎"
  },
  {
    "objectID": "slides.html#background",
    "href": "slides.html#background",
    "title": "wordplay 🎮 💬",
    "section": "Background",
    "text": "Background\nWhat started as some simple modifications to Andrej Karpathy's nanoGPT has now grown into the wordplay project.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: nanoGPT\n\n\n\n\n\n\n\n\n\n\nFigure 2: wordplay\n\n\n\n\n\n\n\nIf you’re curious…\n\nWhile nanoGPT is a great project and an excellent resource; it is, by design, very minimal1 and limited in its flexibility.\nWorking through the code I found myself making minor changes here and there to test new ideas and run variations on different experiments. These changes eventually built to the point where my {goals, scope, code} for the project had diverged significantly from the original vision.\nAs a result, I figured it made more sense to move things to a new project, wordplay.\nI’ve priortized adding functionality that I have found to be useful or interesting, but am absolutely open to input or suggestions for improvement.\nDifferent aspects of this project have been motivated by some of my recent work on LLMs.\n\nProjects:\n\nezpz: Painless distributed training with your favorite {framework, backend} combo.\nMegatron-DeepSpeed: Ongoing research training transformer language models at scale, including: BERT & GPT-2\n\nCollaboration(s):\n\nDeepSpeed4Science (2023-09)\n\nLoooooooong Sequence Lengths\nProject Website\nPreprint Song et al. (2023)\nBlog Post\nTutorial\n\nGenSLMs:\n\nGitHub\nPreprint\n🏆 ACM Gordon Bell Special Prize for COVID-19 Research\n\n\nTalks / Workshops:\n\nLLM-lunch-talk (2023-10-12): LLMs at ALCF.\n\nSlides\nGitHub\n\nCreating Small(-ish) LLMs (2023-11-30)\n\nWorkshop\nSlides\nGitHub\n\n\n\n\n\n\n\n\nnano, even 😂↩︎"
  },
  {
    "objectID": "slides.html#completed",
    "href": "slides.html#completed",
    "title": "wordplay 🎮 💬",
    "section": "Completed",
    "text": "Completed\n\nWork with any 🤗 HuggingFace dataset\nEffortless distributed training using ezpz\nImproved (type-safe) and extensible configuration system (powered by hydra), see #config\nAutomatic, detailed experiment + metric tracking with Weights & Biases\n\nExample Workspace\nExample Run\n\nRich informative logging with enrich"
  },
  {
    "objectID": "slides.html#in-progress",
    "href": "slides.html#in-progress",
    "title": "wordplay 🎮 💬",
    "section": "In Progress",
    "text": "In Progress\n\nDeepSpeed support\nFull-Sharded Data-Parallel (FSDP) support\n\nIntroducing PyTorch Fully Sharded Data Parallel (FSDP) API | PyTorch\n\n3D Parallelism support via:\n\nMegatron-DeepSpeed\nnative PyTorch:\n\nPipeline Parallelism — PyTorch 2.1 documentation\npytorch/PiPPy: Pipeline Parallelism for PyTorch"
  },
  {
    "objectID": "slides.html#install",
    "href": "slides.html#install",
    "title": "wordplay 🎮 💬",
    "section": "Install",
    "text": "Install\n\n\nGrab-n-Go\n\nThe easiest way to get the most recent version is to:\npython3 -m pip install \"git+https://github.com/saforem2/wordplay.git\"\n\n\n\nDevelopment\n\nIf you’d like to work with the project and run / change things yourself, I’d recommend installing from a local (editable) clone of this repository:\ngit clone \"https://github.com/saforem2/wordplay\"\ncd wordplay\nmkdir v venv\npython3 -m venv venv --system-site-packages\nsource venv/bin/activate\npython3 -m pip install -e ."
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "wordplay 🎮 💬",
    "section": "",
    "text": "Last Updated: 12/20/2023 @ 09:11:10\n\n\n\n\n\n\n\n\nSong, Shuaiwen Leon, Bonnie Kruft, Minjia Zhang, Conglong Li, Shiyang Chen, Chengming Zhang, Masahiro Tanaka, et al. 2023. “DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery Through Sophisticated AI System Technologies.” https://arxiv.org/abs/2310.04610."
  }
]