[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "wordplay ğŸ® ğŸ’¬",
    "section": "",
    "text": "Playing with words.\nA set of simple, scalable and highly configurable tools for working1 with LLMs."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "wordplay ğŸ® ğŸ’¬",
    "section": "Background",
    "text": "Background\nWhat started as some simple modifications to Andrej Karpathy's nanoGPT has now grown into the wordplay project.\n\n\n\n\n\n\n\n\n\n\n\n\nFigureÂ 1: nanoGPT\n\n\n\n\n\n\n\n\n\n\nFigureÂ 2: wordplay\n\n\n\n\n\n\n\nIf youâ€™re curiousâ€¦\n\nWhile nanoGPT is a great project and an excellent resource; it is, by design, very minimal2 and limited in its flexibility.\nWorking through the code I found myself making minor changes here and there to test new ideas and run variations on different experiments. These changes eventually built to the point where my {goals, scope, code} for the project had diverged significantly from the original vision.\nAs a result, I figured it made more sense to move things to a new project, wordplay.\nIâ€™ve priortized adding functionality that I have found to be useful or interesting, but am absolutely open to input or suggestions for improvement.\nDifferent aspects of this project have been motivated by some of my recent work on LLMs.\n\nProjects:\n\nezpz: Painless distributed training with your favorite {framework, backend} combo.\nMegatron-DeepSpeed: Ongoing research training transformer language models at scale, including: BERT & GPT-2\n\nCollaboration(s):\n\nDeepSpeed4Science (2023-09)\n\nLoooooooong Sequence Lengths\nProject Website\nPreprint Song et al. (2023)\nBlog Post\nTutorial\n\nGenSLMs:\n\nGitHub\nPreprint\nğŸ† ACM Gordon Bell Special Prize for COVID-19 Research\n\n\nTalks / Workshops:\n\nLLM-lunch-talk (2023-10-12): LLMs at ALCF.\n\nSlides\nGitHub\n\nCreating Small(-ish) LLMs (2023-11-30)\n\nWorkshop\nSlides\nGitHub"
  },
  {
    "objectID": "index.html#completed",
    "href": "index.html#completed",
    "title": "wordplay ğŸ® ğŸ’¬",
    "section": "Completed",
    "text": "Completed\n\nWork with any ğŸ¤— HuggingFace dataset\nEffortless distributed training using ezpz\nImproved (type-safe) and extensible configuration system (powered by hydra), see #config\nAutomatic, detailed experiment + metric tracking with Weights & Biases\n\nExample Workspace\nExample Run\n\nRich informative logging with enrich"
  },
  {
    "objectID": "index.html#in-progress",
    "href": "index.html#in-progress",
    "title": "wordplay ğŸ® ğŸ’¬",
    "section": "In Progress",
    "text": "In Progress\n\nDeepSpeed support\nFull-Sharded Data-Parallel (FSDP) support\n\nIntroducing PyTorch Fully Sharded Data Parallel (FSDP) API | PyTorch\n\n3D Parallelism support via:\n\nMegatron-DeepSpeed\nnative PyTorch:\n\nPipeline Parallelism â€” PyTorch 2.1 documentation\npytorch/PiPPy: Pipeline Parallelism for PyTorch"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "wordplay ğŸ® ğŸ’¬",
    "section": "Install",
    "text": "Install\n\n\nGrab-n-Go\n\nThe easiest way to get the most recent version is to:\npython3 -m pip install \"git+https://github.com/saforem2/wordplay.git\"\n\n\n\nDevelopment\n\nIf youâ€™d like to work with the project and run / change things yourself, Iâ€™d recommend installing from a local (editable) clone of this repository:\ngit clone \"https://github.com/saforem2/wordplay\"\ncd wordplay\nmkdir v venv\npython3 -m venv venv --system-site-packages\nsource venv/bin/activate\npython3 -m pip install -e .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLast Updated: 12/20/2023 @ 09:11:10"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "wordplay ğŸ® ğŸ’¬",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\n{\n  \"training\",\n  \"fine-tuning\",\n  \"benchmarking\",\n  \"parallelizing\",\n  \"distributing\",\n  \"measuring\",\n  \"...\"\n}\nlarge models at scale.â†©ï¸\nnano, even ğŸ˜‚â†©ï¸"
  },
  {
    "objectID": "slides.html#background",
    "href": "slides.html#background",
    "title": "wordplay ğŸ® ğŸ’¬",
    "section": "Background",
    "text": "Background\nWhat started as some simple modifications to Andrej Karpathy's nanoGPT has now grown into the wordplay project.\n\n\n\n\n\n\n\n\n\n\n\n\nFigureÂ 1: nanoGPT\n\n\n\n\n\n\n\n\n\n\nFigureÂ 2: wordplay\n\n\n\n\n\n\n\nIf youâ€™re curiousâ€¦\n\nWhile nanoGPT is a great project and an excellent resource; it is, by design, very minimal1 and limited in its flexibility.\nWorking through the code I found myself making minor changes here and there to test new ideas and run variations on different experiments. These changes eventually built to the point where my {goals, scope, code} for the project had diverged significantly from the original vision.\nAs a result, I figured it made more sense to move things to a new project, wordplay.\nIâ€™ve priortized adding functionality that I have found to be useful or interesting, but am absolutely open to input or suggestions for improvement.\nDifferent aspects of this project have been motivated by some of my recent work on LLMs.\n\nProjects:\n\nezpz: Painless distributed training with your favorite {framework, backend} combo.\nMegatron-DeepSpeed: Ongoing research training transformer language models at scale, including: BERT & GPT-2\n\nCollaboration(s):\n\nDeepSpeed4Science (2023-09)\n\nLoooooooong Sequence Lengths\nProject Website\nPreprint Song et al. (2023)\nBlog Post\nTutorial\n\nGenSLMs:\n\nGitHub\nPreprint\nğŸ† ACM Gordon Bell Special Prize for COVID-19 Research\n\n\nTalks / Workshops:\n\nLLM-lunch-talk (2023-10-12): LLMs at ALCF.\n\nSlides\nGitHub\n\nCreating Small(-ish) LLMs (2023-11-30)\n\nWorkshop\nSlides\nGitHub\n\n\n\n\n\n\n\n\nnano, even ğŸ˜‚â†©ï¸"
  },
  {
    "objectID": "slides.html#completed",
    "href": "slides.html#completed",
    "title": "wordplay ğŸ® ğŸ’¬",
    "section": "Completed",
    "text": "Completed\n\nWork with any ğŸ¤— HuggingFace dataset\nEffortless distributed training using ezpz\nImproved (type-safe) and extensible configuration system (powered by hydra), see #config\nAutomatic, detailed experiment + metric tracking with Weights & Biases\n\nExample Workspace\nExample Run\n\nRich informative logging with enrich"
  },
  {
    "objectID": "slides.html#in-progress",
    "href": "slides.html#in-progress",
    "title": "wordplay ğŸ® ğŸ’¬",
    "section": "In Progress",
    "text": "In Progress\n\nDeepSpeed support\nFull-Sharded Data-Parallel (FSDP) support\n\nIntroducing PyTorch Fully Sharded Data Parallel (FSDP) API | PyTorch\n\n3D Parallelism support via:\n\nMegatron-DeepSpeed\nnative PyTorch:\n\nPipeline Parallelism â€” PyTorch 2.1 documentation\npytorch/PiPPy: Pipeline Parallelism for PyTorch"
  },
  {
    "objectID": "slides.html#install",
    "href": "slides.html#install",
    "title": "wordplay ğŸ® ğŸ’¬",
    "section": "Install",
    "text": "Install\n\n\nGrab-n-Go\n\nThe easiest way to get the most recent version is to:\npython3 -m pip install \"git+https://github.com/saforem2/wordplay.git\"\n\n\n\nDevelopment\n\nIf youâ€™d like to work with the project and run / change things yourself, Iâ€™d recommend installing from a local (editable) clone of this repository:\ngit clone \"https://github.com/saforem2/wordplay\"\ncd wordplay\nmkdir v venv\npython3 -m venv venv --system-site-packages\nsource venv/bin/activate\npython3 -m pip install -e ."
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "wordplay ğŸ® ğŸ’¬",
    "section": "",
    "text": "Last Updated: 12/20/2023 @ 09:11:10\n\n\n\n\n\n\n\n\nSong, Shuaiwen Leon, Bonnie Kruft, Minjia Zhang, Conglong Li, Shiyang Chen, Chengming Zhang, Masahiro Tanaka, et al. 2023. â€œDeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery Through Sophisticated AI System Technologies.â€ https://arxiv.org/abs/2310.04610."
  }
]